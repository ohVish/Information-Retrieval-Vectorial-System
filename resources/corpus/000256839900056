Cortical control of a prosthetic arm for self-feeding
Arm movement is well represented in populations of neurons recorded from the motor cortex(1-7). Cortical activity patterns have been used in the new field of brain-machine interfaces(8-11) to show how cursors on computer displays can be moved in two- and three-dimensional space(12-22). Although the ability to move a cursor can be useful in its own right, this technology could be applied to restore arm and hand function for amputees and paralysed persons. However, the use of cortical signals to control a multi-jointed prosthetic device for direct real-time interaction with the physical environment ('embodiment') has not been demonstrated. Here we describe a system that permits embodied prosthetic control; we show how monkeys (Macaca mulatta) use their motor cortical activity to control a mechanized arm replica in a self-feeding task. In addition to the three dimensions of movement, the subjects' cortical signals also proportionally controlled a gripper on the end of the arm. Owing to the physical interaction between the monkey, the robotic arm and objects in the workspace, this new task presented a higher level of difficulty than previous virtual (cursor-control) experiments. Apart from an example of simple one-dimensional control(23), previous experiments have lacked physical interaction even in cases where a robotic arm(16,19,24) or hand(20) was included in the control loop, because the subjects did not use it to interact with physical objects - an interaction that cannot be fully simulated. This demonstration of multi-degree-of-freedom embodied prosthetic control paves the way towards the development of dexterous prosthetic devices that could ultimately achieve arm and hand function at a near-natural level.
