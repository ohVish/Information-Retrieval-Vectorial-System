Optimization of lag time underlies antibiotic tolerance in evolved bacterial populations
The great therapeutic achievements of antibiotics have been dramatically undercut by the evolution of bacterial strategies that overcome antibiotic stress(1),(2). These strategies fall into two classes. 'Resistance' makes it possible for a microorganism to grow in the constant presence of the antibiotic, provided that the concentration of the antibiotic is not too high. 'Tolerance' allows a microorganism to survive antibiotic treatment, even at high antibiotic concentrations, as long as the duration of the treatment is limited. Although both resistance and tolerance are important reasons for the failure of antibiotic treatments(3-6), the evolution of resistance(7-9) is much better understood than that of tolerance. Here we followed the evolution of bacterial populations under intermittent exposure to the high concentrations of antibiotics used in the clinic and characterized the evolved strains in terms of both resistance and tolerance. We found that all strains adapted by specific genetic mutations, which became fixed in the evolved populations. By monitoring the phenotypic changes at the population and single-cell levels, we found that the first adaptive change to antibiotic stress was the development of tolerance through a major adjustment in the single-cell lag-time distribution, without a change in resistance. Strikingly, we found that the lag time of bacteria before regrowth was optimized to match the duration of the antibiotic-exposure interval. Whole genome sequencing of the evolved strains and restoration of the wild-type alleles allowed us to identify target genes involved in this antibiotic-driven phenotype: 'tolerance by lag' (tbl). Better understanding of lag-time evolution as a key determinant of the survival of bacterial populations under high antibiotic concentrations could lead to new approaches to impeding the evolution of antibiotic resistance.
